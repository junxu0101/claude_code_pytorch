{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Text Classifier for IMDb Movie Reviews\n",
    "\n",
    "A comprehensive implementation of a BERT-based text classifier for sentiment analysis using PyTorch and the Transformers library.\n",
    "\n",
    "**Features:**\n",
    "- Pre-trained BERT model for text classification\n",
    "- IMDb movie reviews dataset (25,000 training + 25,000 test samples)\n",
    "- Comprehensive evaluation metrics\n",
    "- Training visualization and analysis\n",
    "- Example predictions with confidence scores\n",
    "\n",
    "**🤖 Generated by [Claude Code](https://claude.ai/code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T04:33:49.110431Z",
     "start_time": "2025-07-24T04:33:49.108965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision transformers datasets scikit-learn matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T04:47:43.519335Z",
     "start_time": "2025-07-24T04:47:43.508832Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "def get_device():    \n",
    "    if torch.backends.mps.is_available():\n",
    "        # device = torch.device('mps')\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Detected Apple Silicon MPS GPU, however, the model produces NaNs on MPS backend, so, still use CPU\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:16:56.215695Z",
     "start_time": "2025-07-24T05:16:47.711031Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_imdb_dataset():\n",
    "    \"\"\"Load and prepare IMDb dataset\"\"\"\n",
    "    print(\"Loading IMDb dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load from local cache first\n",
    "        if os.path.exists('imdb_data.csv'):\n",
    "            print(\"Loading cached IMDb data...\")\n",
    "            df = pd.read_csv('imdb_data.csv')\n",
    "        else:\n",
    "            # Download IMDb dataset using datasets library\n",
    "            from datasets import load_dataset\n",
    "            print(\"Downloading IMDb dataset from Hugging Face...\")\n",
    "            dataset = load_dataset('imdb')\n",
    "            \n",
    "            # Convert to pandas DataFrame\n",
    "            train_data = dataset['train'].to_pandas()\n",
    "            test_data = dataset['test'].to_pandas()\n",
    "            \n",
    "            # Combine and shuffle\n",
    "            df = pd.concat([train_data, test_data], ignore_index=True)\n",
    "            df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Cache the data\n",
    "            df.to_csv('imdb_data.csv', index=False)\n",
    "            print(f\"Cached dataset to imdb_data.csv\")\n",
    "        \n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Total samples: {len(df)}\")\n",
    "        print(f\"Positive samples: {df['label'].sum()}\")\n",
    "        print(f\"Negative samples: {len(df) - df['label'].sum()}\")\n",
    "        \n",
    "        return df['text'].values, df['label'].values\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(\"datasets library not available.\")\n",
    "        # print(\"Creating synthetic IMDb-like data...\")\n",
    "        raise e\n",
    "        # return create_synthetic_data()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading IMDb dataset.\")\n",
    "        # print(\"Creating synthetic IMDb-like data...\")\n",
    "        raise e\n",
    "        # return create_synthetic_data()\n",
    "\n",
    "# Claude Code generated this code but it does not make sense to continue with synthetic data if the real data set cannot downloaded.\n",
    "#\n",
    "# def create_synthetic_data():\n",
    "#     \"\"\"Create synthetic movie review data for demonstration\"\"\"\n",
    "#     positive_reviews = [\n",
    "#         \"This movie was absolutely fantastic! The acting was superb and the plot was engaging throughout.\",\n",
    "#         \"I loved every minute of this film. The cinematography was breathtaking and the story was compelling.\",\n",
    "#         \"An incredible masterpiece! The characters were well-developed and the ending was perfect.\",\n",
    "#         \"Outstanding performance by all actors. This is definitely one of the best movies I've ever seen.\",\n",
    "#         \"Brilliant storytelling and amazing special effects. Highly recommended!\",\n",
    "#         \"A wonderful film that kept me on the edge of my seat. Great direction and excellent music.\",\n",
    "#         \"This movie exceeded all my expectations. The dialogue was witty and the pacing was perfect.\",\n",
    "#         \"Absolutely loved it! The plot twists were unexpected and the acting was phenomenal.\",\n",
    "#         \"A true gem of cinema. The emotional depth and visual effects were remarkable.\",\n",
    "#         \"Perfect blend of action and drama. The cast delivered outstanding performances.\"\n",
    "#     ] * 250  # Repeat to create more samples\n",
    "#\n",
    "#     negative_reviews = [\n",
    "#         \"This movie was terrible. The plot made no sense and the acting was awful.\",\n",
    "#         \"I couldn't even finish watching this film. It was boring and poorly executed.\",\n",
    "#         \"Worst movie I've ever seen. The dialogue was cringe-worthy and the story was predictable.\",\n",
    "#         \"A complete waste of time. The characters were one-dimensional and the plot was confusing.\",\n",
    "#         \"Poorly directed and badly acted. I want my money back.\",\n",
    "#         \"This film was a disaster. The pacing was terrible and the ending was disappointing.\",\n",
    "#         \"Absolutely horrible. The special effects were cheap and the story made no sense.\",\n",
    "#         \"I fell asleep halfway through. The movie was boring and the characters were unlikeable.\",\n",
    "#         \"A terrible film with poor writing and unconvincing performances.\",\n",
    "#         \"One of the worst movies ever made. The plot holes were enormous and the acting was wooden.\"\n",
    "#     ] * 250  # Repeat to create more samples\n",
    "#\n",
    "#     # Combine positive (1) and negative (0) reviews\n",
    "#     reviews = positive_reviews + negative_reviews\n",
    "#     labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "#\n",
    "#     # Shuffle the data\n",
    "#     combined = list(zip(reviews, labels))\n",
    "#     np.random.shuffle(combined)\n",
    "#     reviews, labels = zip(*combined)\n",
    "#\n",
    "#     print(f\"Created synthetic dataset with {len(reviews)} samples\")\n",
    "#     return np.array(reviews), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "reviews, labels = load_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:18:13.608027Z",
     "start_time": "2025-07-24T05:18:13.603237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display some sample reviews\n",
    "print(\"Sample Reviews:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    sentiment = \"Positive\" if labels[i] == 1 else \"Negative\"\n",
    "    print(f\"\\n{i+1}. Label: {sentiment}\")\n",
    "    print(f\"Review: {reviews[i][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:42:52.724877Z",
     "start_time": "2025-07-25T22:42:52.707408Z"
    }
   },
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for IMDb reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=512):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize and encode the review\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"IMDbDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT Text Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:55:09.520696Z",
     "start_time": "2025-07-25T22:55:09.508075Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTTextClassifier(nn.Module):\n",
    "    \"\"\"BERT-based text classifier for sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=2, bert_model_name='bert-base-uncased', dropout=0.3):\n",
    "        super(BERTTextClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # print(\"\\n--- Forward Pass Debugging ---\")\n",
    "        # print(f\"Input_ids min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "        # print(f\"Attention_mask min: {attention_mask.min().item()}, max: {attention_mask.max().item()}\")\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # # --- Check pooled_output ---\n",
    "        # if torch.isnan(pooled_output).any():\n",
    "        #     print(\"pooled_output has NaNs! Problem likely in BertModel output.\")\n",
    "        # if torch.isinf(pooled_output).any():\n",
    "        #     print(\"pooled_output has Infs! Problem likely in BertModel output.\")\n",
    "        # print(f\"Pooled_output min: {pooled_output.min().item():.4f}, max: {pooled_output.max().item():.4f}, mean: {pooled_output.mean().item():.4f}, std: {pooled_output.std().item():.4f}\")\n",
    "\n",
    "        output = self.dropout(pooled_output)\n",
    "\n",
    "        # # --- Check output_after_dropout ---\n",
    "        # if torch.isnan(output).any():\n",
    "        #     print(\"output_after_dropout has NaNs! Problem likely introduced by dropout.\")\n",
    "        # if torch.isinf(output).any():\n",
    "        #     print(\"output_after_dropout has Infs! Problem likely introduced by dropout.\")\n",
    "        # print(f\"Output after dropout min: {output.min().item():.4f}, max: {output.max().item():.4f}, mean: {output.mean().item():.4f}, std: {output.std().item():.4f}\")\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        \n",
    "        # # --- Check final logits ---\n",
    "        # if torch.isnan(logits).any():\n",
    "        #     print(\"Final logits have NaNs! Problem likely introduced by classifier (nn.Linear).\")\n",
    "        # if torch.isinf(logits).any():\n",
    "        #     print(\"Final logits have Infs! Problem likely introduced by classifier (nn.Linear).\")\n",
    "        # print(f\"Logits min: {logits.min().item():.4f}, max: {logits.max().item():.4f}, mean: {logits.mean().item():.4f}, std: {logits.std().item():.4f}\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "print(\"BERTTextClassifier model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:11:18.607117Z",
     "start_time": "2025-07-26T00:11:18.582534Z"
    }
   },
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Track training and validation metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.epoch_times = []\n",
    "        self.learning_rates = []\n",
    "    \n",
    "    def update(self, train_loss, val_loss, train_acc, val_acc, epoch_time, lr):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.val_accuracies.append(val_acc)\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on given data\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "\n",
    "        # debugging NaN issue\n",
    "        # if torch.isnan(batch['input_ids']).any(): print(\"NaN in input_ids!\")\n",
    "        # if torch.isinf(batch['input_ids']).any(): print(\"Inf in input_ids!\")\n",
    "        # if torch.isnan(batch['attention_mask']).any(): print(\"NaN in attention_mask!\")\n",
    "        # if torch.isinf(batch['attention_mask']).any(): print(\"Inf in attention_mask!\")\n",
    "        # if torch.isnan(batch['label']).any(): print(\"NaN in labels!\")\n",
    "        # if torch.isinf(batch['label']).any(): print(\"Inf in labels!\")\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # # --- Add this line for debugging ---\n",
    "        # print(f\"Logits min: {outputs.min().item():.4f}, max: {outputs.max().item():.4f}, mean: {outputs.mean().item():.4f}, std: {outputs.std().item():.4f}\")\n",
    "        # # ----------------------------------\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f'  Batch {i}/{len(data_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training and evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preparation and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:15:20.905776Z",
     "start_time": "2025-07-26T00:15:20.628681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"Max Length: {MAX_LENGTH}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"BERT Model: {BERT_MODEL_NAME}\")\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    reviews, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:17:07.699422Z",
     "start_time": "2025-07-26T00:17:05.930680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(f\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IMDbDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
    "val_dataset = IMDbDataset(X_val, y_val, tokenizer, MAX_LENGTH)\n",
    "test_dataset = IMDbDataset(X_test, y_test, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Data loaders created successfully!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:34:39.884308Z",
     "start_time": "2025-07-26T00:34:29.031834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(f\"Loading BERT model...\")\n",
    "model = BERTTextClassifier(n_classes=2, bert_model_name=BERT_MODEL_NAME)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Model and optimizer initialized successfully!\")\n",
    "print(f\"Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:44:23.060585Z",
     "start_time": "2025-07-26T00:44:16.707545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics tracker\n",
    "metrics_tracker = MetricsTracker()\n",
    "\n",
    "# Training loop\n",
    "print(f\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, device\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Update metrics\n",
    "    metrics_tracker.update(train_loss, val_loss, train_acc, val_acc, epoch_time, current_lr)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "    print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), 'best_bert_model.pth')\n",
    "        print(\"New best model saved!\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {total_training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "model.load_state_dict(torch.load('best_bert_model.pth'))\n",
    "\n",
    "# Test evaluation\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_acc, test_predictions, test_labels = evaluate_model(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "# Calculate detailed metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_predictions, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "class_names = ['Negative', 'Positive']\n",
    "print(classification_report(test_labels, test_predictions, \n",
    "                          target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(metrics_tracker):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    epochs = range(1, len(metrics_tracker.train_losses) + 1)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(epochs, metrics_tracker.train_losses, 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, metrics_tracker.val_losses, 'r-', label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(epochs, metrics_tracker.train_accuracies, 'b-', label='Training Accuracy')\n",
    "    ax2.plot(epochs, metrics_tracker.val_accuracies, 'r-', label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    ax3.plot(epochs, metrics_tracker.learning_rates, 'g-', label='Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Training Time\n",
    "    ax4.plot(epochs, metrics_tracker.epoch_times, 'purple', label='Epoch Time')\n",
    "    ax4.set_title('Training Time per Epoch')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Time (seconds)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bert_training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('bert_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_training_history(metrics_tracker)\n",
    "plot_confusion_matrix(test_labels, test_predictions, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text, device, max_length=512):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    confidence = torch.max(predictions, dim=1)[0].item()\n",
    "    predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example predictions\n",
    "print(f\"Example Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_examples = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible film, waste of time. Poor acting and boring plot.\",\n",
    "    \"An okay movie, nothing special but not bad either.\",\n",
    "    \"Brilliant cinematography and outstanding performances by all actors.\",\n",
    "    \"I fell asleep halfway through. Very disappointing.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    pred_class, confidence = predict_sentiment(model, tokenizer, text, device)\n",
    "    sentiment = \"Positive\" if pred_class == 1 else \"Negative\"\n",
    "    print(f\"\\n{i}. Review: {text}\")\n",
    "    print(f\"   Predicted: {sentiment} (Confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n🎉 BERT Text Classification Pipeline Completed Successfully!\")\n",
    "print(f\"📊 Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"⏱️ Total Training Time: {total_training_time/60:.2f} minutes\")\n",
    "print(f\"\\n📈 Final Results Summary:\")\n",
    "print(f\"   • Model: {BERT_MODEL_NAME}\")\n",
    "print(f\"   • Dataset: IMDb Movie Reviews\")\n",
    "print(f\"   • Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   • Test F1-Score: {f1:.4f}\")\n",
    "print(f\"   • Total Parameters: {total_params:,}\")\n",
    "print(f\"   • Training Device: {device}\")\n",
    "\n",
    "print(f\"\\n💾 Saved Files:\")\n",
    "print(f\"   • best_bert_model.pth - Trained model weights\")\n",
    "print(f\"   • bert_training_metrics.png - Training visualization\")\n",
    "print(f\"   • bert_confusion_matrix.png - Confusion matrix\")\n",
    "print(f\"   • imdb_data.csv - Cached dataset (if downloaded)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
